{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expressive Gestures\n",
    "\n",
    "Laban/Bartenieff Movement Analysis (LMA) characterizes gestures as individual movements that convey meaning. In LMA, expressive gestures are defined as movements that convey emotions, intentions, or personal artistic expression, often characterized by their dynamic qualities, such as flow, weight, time, and space, which reflect the performer's inner state or interpretation of a theme. In this notebook, the same behavioral gestures will be analyzed to extract expressive qualities within these movements.\n",
    "\n",
    "### Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pose_abs = pd.read_csv('./data/interim/landmarks/contemporary_dance_solo_20230219225812_10.csv')\n",
    "df_pose_rel = pd.read_csv('./data/interim/landmarks/contemporary_dance_solo_20230219225812_10.csv')\n",
    "path_gestures_stand = sorted(os.listdir('.data/interim/dance_video_name/gestures/stand/'))\n",
    "path_gestures_sitdown = sorted(os.listdir('.data/interim/dance_video_name/gestures/sitdown/'))\n",
    "path_gestures_bentknee = sorted(os.listdir('.data/interim/dance_video_name/gestures/bentknee/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing Variations\n",
    "\n",
    "The aim of this section is to analyze the duration, rhythm, and tempo of the behavioral gestures to identify any variations that might indicate emotional expression. For example, a slower or faster pace in standing or sitting might reflect the performer's mood or intention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the duration of each gesture type\n",
    "duration_stand = len(path_gestures_stand)\n",
    "duration_sitdown = len(path_gestures_sitdown)\n",
    "duration_bentknee = len(path_gestures_bentknee)\n",
    "\n",
    "# Calculate the tempo of each gesture type\n",
    "frame_rate = 24\n",
    "tempo_stand = duration_stand / frame_rate\n",
    "tempo_sitdown = duration_sitdown / frame_rate\n",
    "tempo_bentknee = duration_bentknee / frame_rate\n",
    "\n",
    "# Calculate the rhythm (time between gestures) assuming uniform spacing between gestures\n",
    "rhythm_stand = 0 if len(path_gestures_stand) <= 1 else (path_gestures_stand[-1] - path_gestures_stand[0]) / (len(path_gestures_stand) - 1)\n",
    "rhythm_sitdown = 0 if len(path_gestures_sitdown) <= 1 else (path_gestures_sitdown[-1] - path_gestures_sitdown[0]) / (len(path_gestures_sitdown) - 1)\n",
    "rhythm_bentknee = 0 if len(path_gestures_bentknee) <= 1 else (path_gestures_bentknee[-1] - path_gestures_bentknee[0]) / (len(path_gestures_bentknee) - 1)\n",
    "\n",
    "print(\"Duration (Stand, Sitdown, Bent Knee):\", duration_stand, duration_sitdown, duration_bentknee)\n",
    "print(\"Tempo (Stand, Sitdown, Bent Knee):\", tempo_stand, tempo_sitdown, tempo_bentknee)\n",
    "print(\"Rhythm (Stand, Sitdown, Bent Knee):\", rhythm_stand, rhythm_sitdown, rhythm_bentknee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow and Dynamics\n",
    "\n",
    "The aim of this section is to examine the smoothness, continuity, and intensity of movements during the execution of behavioral gestures, as they may reveal the performer's emotional state. The following code calculates flow values for each frame in the behavioral gestures by measuring the Euclidean distance between the pose keypoints in consecutive frames, and then calculates the mean, standard deviation, minimum, and maximum flow values for each gesture to provide insights into the smoothness, continuity, and intensity of the movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the Euclidean distance between two points\n",
    "def euclidean_distance(p1, p2):\n",
    "    return np.sqrt(np.sum((p1 - p2) ** 2))\n",
    "\n",
    "# Create an empty DataFrame to store the flow and dynamics information\n",
    "flow_dynamics_df = pd.DataFrame(columns=['Gesture', 'Frame', 'Flow'])\n",
    "\n",
    "# Loop through each gesture directory and calculate flow values for each frame\n",
    "gesture_directories = ['stand', 'sitdown', 'bentknee']\n",
    "for gesture in gesture_directories:\n",
    "    gesture_path = f'.data/interim/dance_video_name/gestures/{gesture}/'\n",
    "    image_files = sorted(os.listdir(gesture_path))\n",
    "    \n",
    "    prev_coordinates = None\n",
    "    for i, image_file in enumerate(image_files[:-1]):\n",
    "        current_frame = int(image_file.split('.')[0])\n",
    "        next_frame = int(image_files[i + 1].split('.')[0])\n",
    "        \n",
    "        current_coordinates = df_pose_abs[df_pose_abs['frame'] == current_frame].iloc[:, 1:].values\n",
    "        next_coordinates = df_pose_abs[df_pose_abs['frame'] == next_frame].iloc[:, 1:].values\n",
    "        \n",
    "        if prev_coordinates is not None:\n",
    "            flow = euclidean_distance(prev_coordinates, current_coordinates)\n",
    "            flow_dynamics_df = flow_dynamics_df.append({\n",
    "                'Gesture': gesture,\n",
    "                'Frame': current_frame,\n",
    "                'Flow': flow\n",
    "            }, ignore_index=True)\n",
    "        \n",
    "        prev_coordinates = next_coordinates\n",
    "\n",
    "# Calculate statistics for each gesture\n",
    "flow_dynamics_summary = flow_dynamics_df.groupby('Gesture').agg({\n",
    "    'Flow': ['mean', 'std', 'min', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "print(flow_dynamics_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body Posture and Alignment\n",
    "\n",
    "The aim of this section is to assess the posture and alignment of the body during the performance of behavioral gestures. Differences in body posture can indicate the performer's emotions, intentions, or personal style. Here are the potential insights per metric:\n",
    "\n",
    "* **Mean per relevant joint**: This data can be used to compare other gestures or to the ideal posture to see if there are any significant deviations.\n",
    "* **Standard deviation per relevant joint**: High variability may indicate that the gesture is more difficult to execute or that the performer is expressing more emotion.\n",
    "* **Calculated Range of Motion**: This data can be used to compare with other gestures or to the ideal posture to see if there are any significant differences.\n",
    "* **Average Range of Motion**: Higher average range of motion may indicate that the performer is expressing more emotion or that the gesture is more dynamic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the joint columns to use for posture and alignment analysis\n",
    "standing_joint_columns = [col for col in df_pose_abs.columns if col not in ['frame', 'landmarks', 'torso_x', 'torso_y', 'torso_z']]\n",
    "standing_joint_mean = df_pose_abs[joint_columns].mean()\n",
    "standing_joint_std = df_pose_abs[joint_columns].std()\n",
    "standing_joint_max = df_pose_abs[joint_columns].max()\n",
    "standing_joint_min = df_pose_abs[joint_columns].min()\n",
    "\n",
    "# Calculate the range of motion (RoM)\n",
    "standing_joint_range = standing_joint_max - standing_joint_min\n",
    "standing_avg_range = np.mean(joint_range)\n",
    "\n",
    "# Calculate the average posture and alignment values across all frames\n",
    "standing_posture_mean = np.mean(joint_mean)\n",
    "standing_alignment_mean = np.mean(joint_mean[['left_hip', 'right_hip']] - joint_mean[['left_shoulder', 'right_shoulder']])\n",
    "\n",
    "print(f\"Mean per relevant joint: {standing_joint_mean})\n",
    "print(f\"STD per relevant joint: {standing_joint_mean})\n",
    "print(f\"Calculated range of motion: {standing_joint_range})\n",
    "print(f\"Average Range of Motion: {standing_avg_range})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Patterns\n",
    "\n",
    "The aim of this section is to visualize the spatial patterns and range of motion during the performance of behavioral gestures. These are the potential insights:\n",
    "\n",
    "* Identification of specific joint movements that are consistently used during a particular gesture\n",
    "* Differences in joint movements and range of motion between different performers or performances\n",
    "* Correlation between certain joint movements and emotional expression conveyed by the gesture\n",
    "* Identification of unusual or unexpected joint movements that could indicate a unique style or interpretation of the gesture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the movement pathways of each joint\n",
    "for joint in standing_joint_columns:\n",
    "    x = df_pose_abs[joint+'_x']\n",
    "    y = df_pose_abs[joint+'_y']\n",
    "    z = df_pose_abs[joint+'_z']\n",
    "    ax.plot(x, y, z, label=joint)\n",
    "\n",
    "# Set the labels and title\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('Spatial Pathways of Joints during Behavioral Gestures')\n",
    "\n",
    "# Set the limits of the plot\n",
    "max_range = np.array([df_pose_abs[joint+'_x'].max()-df_pose_abs[joint+'_x'].min(),\n",
    "                      df_pose_abs[joint+'_y'].max()-df_pose_abs[joint+'_y'].min(),\n",
    "                      df_pose_abs[joint+'_z'].max()-df_pose_abs[joint+'_z'].min()]).max()\n",
    "for axis in 'xyz':\n",
    "    eval('ax.set_{}lim(-max_range, max_range)'.format(axis))\n",
    "\n",
    "# Show the legend and plot\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "(Drafted examples)\n",
    "\n",
    "* **Timing Variations**: The analysis of duration, tempo, and rhythm revealed interesting differences in the pacing of the gestures across the different categories, which could be indicative of the performer's emotional expression.\n",
    "\n",
    "* **Flow and Dynamics**: The examination of the smoothness, continuity, and intensity of movements revealed that certain gestures had higher flow values, indicating more dynamic and expressive movements.\n",
    "\n",
    "* **Body Posture and Alignment**: The assessment of posture and alignment highlighted significant differences in joint movements and range of motion between different gestures, which could be used to identify specific joint movements that are consistently used during a particular gesture.\n",
    "\n",
    "* **Spatial Patterns**: The visualization of spatial patterns and range of motion during the performance of behavioral gestures revealed unique styles and interpretations of the gestures, which could be correlated with emotional expression conveyed by the gesture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
