{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook provides an overview of the data collection process and the approach to preprocessing the kinematic data for the downloaded dance video dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection using Youtube Data API and Pytube\n",
    "\n",
    "The video data used for exploratory data analysis was downloaded using the [Youtube Data API](https://developers.google.com/youtube/v3/docs/search/list) and [Pytube](https://pytube.io/en/latest/), which ensured that only authorized videos were collected for analysis. To increase the likelihood of finding relevant and clean videos that focused on individual dancers rather than groups, the code function used a keyword search that included the genre name and terms such as \"solo choreography\", \"solo practice\", or \"dance cover\". The expected video format is `mp4`, `width:360`, `height:640`, `max_length:120`, `min_views:100`.\n",
    "\n",
    "For more details about the data collection process, please refer to the code in [/src/data/collection.py](https://github.com/kayesokua/gestures/blob/main/src/data/collection.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.collection import extract_video_from_youtube\n",
    "\n",
    "extract_video_from_youtube(query='contemporary', max_count=5)\n",
    "extract_video_from_youtube(query='ballet', max_count=5)\n",
    "extract_video_from_youtube(query='folk', max_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Estimation using MediaPipe\n",
    "\n",
    "After downloading the video data, the kinematic data will be extracted using [MediaPipe Pose Solution](https://github.com/google/mediapipe/blob/master/docs/solutions/pose.md). The chosen output format is `csv` with relative values by default. In this file, we obtain `x`,`y`,`z` coordinates and obtain the `fps` using [OpenCV](https://docs.opencv.org/4.x/). We use `NaN` to frames where a pose cannot be detected. \n",
    "\n",
    "The code snippet below gathers all videos in `mp4` format and extracts landmarks and screenshots. For more details about the data annotation process, please refer to the code in [/src/data/annotation.py](https://github.com/kayesokua/gestures/blob/main/src/data/annotation.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[7831]: Class CaptureDelegate is implemented in both /Users/caijinsi/.pyenv/versions/3.10.6/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x136a7a4d0) and /Users/caijinsi/.pyenv/versions/3.10.6/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x137bd4860). One of the two will be used. Which one is undefined.\n",
      "objc[7831]: Class CVWindow is implemented in both /Users/caijinsi/.pyenv/versions/3.10.6/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x136a7a520) and /Users/caijinsi/.pyenv/versions/3.10.6/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x1315f0a68). One of the two will be used. Which one is undefined.\n",
      "objc[7831]: Class CVView is implemented in both /Users/caijinsi/.pyenv/versions/3.10.6/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x136a7a548) and /Users/caijinsi/.pyenv/versions/3.10.6/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x1315f0a90). One of the two will be used. Which one is undefined.\n",
      "objc[7831]: Class CVSlider is implemented in both /Users/caijinsi/.pyenv/versions/3.10.6/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x136a7a570) and /Users/caijinsi/.pyenv/versions/3.10.6/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x1315f0ab8). One of the two will be used. Which one is undefined.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 3636 frames from classical_ballet_001.mp4 with 492 missing poses\n",
      "Extracted 3636 frames in total\n",
      "Elapsed time: 95.257 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.data.annotation import extract_landmarks_from_videos\n",
    "\n",
    "video_path = 'data/external/test'\n",
    "\n",
    "if os.path.exists(video_path):\n",
    "    extract_landmarks_from_videos(video_path)\n",
    "else:\n",
    "    print(\"Path does not exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Audio Features using Librosa\n",
    "\n",
    "Although the focus of the research is not on the musicality of the performance, extracting audio information using [Librosa](https://librosa.org/doc/latest/index.html) can add meaningful context to the gestures.\n",
    "\n",
    "\n",
    "* **Tempo** and **Beat Times** are important audio features that can be used to identify the rhythmic structure and timing of the music in a dance video. \n",
    "* [**Root Mean Square**](https://librosa.org/doc/main/generated/librosa.feature.rms.html) can indicate the overall level of activity in the music and provide information on the rhythmic complexity, intensity, and expressive qualities of the dance movements.\n",
    "* [**Zero Crossing Rate**](https://librosa.org/doc/main/generated/librosa.feature.zero_crossing_rate.html) can provide information on the rhythmic regularity and complexity of the music, as well as its relationship to the dance movements.\n",
    "\n",
    "For more details about the audio extraction process, please refer to the code in [/src/features/audio.py](https://github.com/kayesokua/gestures/blob/main/src/features/audio.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.audio import extract_tempo_and_beats, extract_rms_energy, extract_zero_crossing_rate\n",
    "\n",
    "video_url_path = 'data/external/contemporary_005.mp4'\n",
    "tempo, beat_times = extract_tempo_and_beats(video_url_path)\n",
    "rms = extract_rms_energy(video_url_path)\n",
    "zcr = extract_zero_crossing_rate(video_url_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Poses and Outlier Detection\n",
    "\n",
    "Since we are handling dance videos with different cinematography style, using linear interpolation or median might not be appropriate for handling missing kinematic data. Therefore, the proposed solution is to detect outliers instead by generating binary label using [Isolation Forest algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html).  `1` indicates normal data and `-1` indicates the outlier.\n",
    "\n",
    "For more details about the data processing, please refer to the code in [/src/data/processing.py](https://github.com/kayesokua/gestures/blob/main/src/data/processing.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.processing import process_landmarks_using_isolation_forest\n",
    "process_landmarks_using_isolation_forest(\"data/interim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Currently, only minimal data cleaning is required, which involves verifying whether all videos are of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.processing import check_video_size\n",
    "\n",
    "if check_video_size('data/external'):\n",
    "    print(\"All videos have the same size.\")\n",
    "else:\n",
    "    print(\"Videos have different sizes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provided an overview of the data we have for exploration\n",
    "\n",
    "1. Videos (MP4) with category as filename: `data/external/{category_i}.mp4`\n",
    "2. Kinematic data with outliers information(CSV): `data/processed/{category_i}.csv`\n",
    "3. Frame screenshots (PNG) saved in chronological order: `data/interim/{video_filename}/*.png`\n",
    "4. Function methods can extract various audio features: `tempo`,`beat_times`, `rms`,`zcr`\n",
    "\n",
    "\n",
    "## Resources\n",
    "\n",
    "For a comprehensive list of resources, [please see here](https://github.com/kayesokua/gestures/blob/main/references/README.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
